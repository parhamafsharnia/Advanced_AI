{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b429df8e-1c3b-4928-a558-d5f42afc2f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.376403Z",
     "start_time": "2023-11-10T12:43:40.964558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a13002-b01f-47fb-98a6-4f2ae1aeeaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.383817Z",
     "start_time": "2023-11-10T12:43:49.379384Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d7f35-cd90-453b-be2a-5a1f9b9b12f9",
   "metadata": {},
   "source": [
    "<b>Question 1</b><br>\n",
    "Write a program that returns the number of times a word is repeated in each sentence using the Gutenberg structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f175af4f-7efc-43cb-abc1-5520868e1292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.441689Z",
     "start_time": "2023-11-10T12:43:49.385809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1486\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist\n",
    "\n",
    "def wordCounter(word):\n",
    "    '''This function get a word as an input and returns a number \n",
    "    that is the number of times the word repeated in Gutenberg corpora'''\n",
    "    \n",
    "    result = 0\n",
    "    for fileid in gutenberg.fileids():\n",
    "        rawTxt = gutenberg.raw(fileid)\n",
    "        result += rawTxt.count(word)\n",
    "    return result\n",
    "\n",
    "def wordCounter1(word):\n",
    "    '''This function will search for a word in each sentence and the result: print sentence, counter'''\n",
    "    for fileid in gutenberg.fileids():\n",
    "        sents = gutenberg.sents(fileid)\n",
    "        for sent in sents:\n",
    "            result = sent.count(word)\n",
    "            print(sent, result)\n",
    "            result = 0\n",
    "\n",
    "word = 'Hi'\n",
    "print(wordCounter(word=word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ac3cc-6d98-4342-ba9b-93fd20fa5413",
   "metadata": {},
   "source": [
    "<b>Question 2</b><br>\n",
    "Define a function find_language() that takes a string as its argument, and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa763c9-2c28-4253-a1e2-47891eb16880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.863736Z",
     "start_time": "2023-11-10T12:43:49.445649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Javanese-Latin1']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "import re\n",
    "\n",
    "def find_language(string, encoding='Latin1'):\n",
    "    \n",
    "    limit = []\n",
    "    lst = []\n",
    "    for fileid in udhr.fileids():\n",
    "        if re.search(encoding, fileid):\n",
    "            limit.append(fileid)\n",
    "            \n",
    "    for lang in limit:\n",
    "        if word in udhr.words(lang):\n",
    "            lst.append(lang)\n",
    "            \n",
    "    return lst\n",
    "\n",
    "word = 'lair'\n",
    "print(find_language(string=word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e22936-a068-4583-983d-ec036542ea1b",
   "metadata": {},
   "source": [
    "<b>Question 3</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee049655-105a-43bf-ba72-b7ec599f68b1",
   "metadata": {},
   "source": [
    "Consider the following list:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fc2573-38f9-4b6b-83f2-bc4bf57e600b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.870806Z",
     "start_time": "2023-11-10T12:43:49.865691Z"
    }
   },
   "outputs": [],
   "source": [
    "email_addresses = [\"john.doe@example.com\",\"alice.smith@gmail.com\",\"user123@yahoo.com\",\"invalid-email\",\"another.invalid@.com\",\"spammer@spam..com\",\"test.mail@domain.com\",\"no-at-sign-domain.com\",\"@missing-username.com\",\"user@valid-domain.net\",\"another.user@valid-domain.org\",\"info@company.com\",\"support@service.net\",\"user@invalid.ir\",\"sales@company.org\",\"contact.us@website.com\",\"random-email@domain.org\",\"user1234@hotmail.com\",\"another.invalid@domain\",\"email@missing-tld.\",\"user@-hyphen-in-domain.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710086d6-eda6-4f23-ad44-461fa5c71691",
   "metadata": {},
   "source": [
    "a. Now, split each email address in <code>email_addresses</code> into its username and domain parts. Store the usernames in a list called <code>usernames</code> and the domains in a list called <code>domains</code>.<br>\n",
    "\n",
    "b. Create a function called <code>validate_email_domain</code> that takes a domain as input and checks if it is a valid domain. For simplicity, consider a valid domain to be one that ends with \".com\", \".ir\", \".org\", or \".net\". Use Regular Expressions for this validation.<br>\n",
    "\n",
    "c. Use the <code>validate_email_domain</code> function to filter out the valid domains. Store the valid domains in a list called <code>valid_domains</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12131502-5f82-428e-9aea-ad918775311f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.888271Z",
     "start_time": "2023-11-10T12:43:49.872314Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af42fd8f-a8a3-405a-bf5d-6e0f358f5951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.916233Z",
     "start_time": "2023-11-10T12:43:49.890266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john.doe@example.com',\n",
       " 'alice.smith@gmail.com',\n",
       " 'user123@yahoo.com',\n",
       " 'invalid-email',\n",
       " 'another.invalid@.com',\n",
       " 'spammer@spam..com',\n",
       " 'test.mail@domain.com',\n",
       " 'no-at-sign-domain.com',\n",
       " '@missing-username.com',\n",
       " 'user@valid-domain.net',\n",
       " 'another.user@valid-domain.org',\n",
       " 'info@company.com',\n",
       " 'support@service.net',\n",
       " 'user@invalid.ir',\n",
       " 'sales@company.org',\n",
       " 'contact.us@website.com',\n",
       " 'random-email@domain.org',\n",
       " 'user1234@hotmail.com',\n",
       " 'another.invalid@domain',\n",
       " 'email@missing-tld.',\n",
       " 'user@-hyphen-in-domain.com']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c86a7c-9f98-4ab8-bac3-5a235d2dfcb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.934149Z",
     "start_time": "2023-11-10T12:43:49.918191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "usernames = []\n",
    "domains = []\n",
    "\n",
    "pattern = '\\S+@\\w+\\.\\w+' # validate emails\n",
    "\n",
    "for email in email_addresses:\n",
    "    if re.findall(pattern, email):\n",
    "        s = re.split(\"@\", email)\n",
    "        usernames.append(s[0])\n",
    "        domains.append(s[1])\n",
    "# b        \n",
    "def validate_email_domain(domain):\n",
    "    valid = ['.com', '.ir', '.org', '.net']\n",
    "    pattern = '\\.\\w+'\n",
    "    if re.findall(pattern, domain)[0] in valid:\n",
    "        return True\n",
    "    return False\n",
    "# c\n",
    "valid_domains = []\n",
    "for domain in domains:    \n",
    "    valid_domains.append(validate_email_domain(domain=domain))\n",
    "valid_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc436b27-6ccc-4cb9-a860-21639cd19035",
   "metadata": {},
   "source": [
    "<b>Question 4</b><br>\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca895bd6-b8ee-491a-9315-414f91b8f648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:49.956091Z",
     "start_time": "2023-11-10T12:43:49.935147Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'mobi', 'dick', 'by', 'herman', 'melvil', '1851', ']', 'etymolog', '.', '(', 'suppli', 'by', 'a', 'late', 'consumpt', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbar', 'in', 'coat', ',', 'heart', ',', 'bodi', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'wa', 'ever', 'dust', 'hi', 'old', 'lexicon', 'and'] ['[', 'moby', 'dick', 'by', 'herm', 'melvil', '1851', ']', 'etymolog', '.', '(', 'supply', 'by', 'a', 'lat', 'consum', 'ush', 'to', 'a', 'gramm', 'school', ')', 'the', 'pal', 'ush', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ev', 'dust', 'his', 'old', 'lexicon', 'and']\n"
     ]
    }
   ],
   "source": [
    "tokens = text1.tokens[:50]\n",
    "from nltk import PorterStemmer\n",
    "from nltk import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "porterLst = []\n",
    "lancasterLst = []\n",
    "\n",
    "for word in tokens:\n",
    "    porterLst.append(porter.stem(word))\n",
    "    lancasterLst.append(lancaster.stem(word))\n",
    "print(porterLst, lancasterLst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6bb63-cc4f-4f00-b44a-83c012f7687a",
   "metadata": {},
   "source": [
    "<b>Question 5</b><br>\n",
    "Design a simple extractive summarization tool that generates a summary of a given text. The summary should consist of 4 sentences for every 10 sentences in the original text. Here's how you can achieve this:<br><br> a. Preprocess the given text to remove any noise, special characters, and irrelevant information.<br> b. Calculate the word frequency for each word <br>c. Break down the text into sentences and compute the total word frequency for each sentence by summing up the frequencies of the words it contains.<br> d. Rank the sentences based on their total word frequency scores.<br> e. Select the top n highest-scoring sentences (Depending on the length of the text, 4, 8 and...) and print them in the order they appear in the original document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3b3fa6-a99c-4744-8e15-bdd73203a1d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.018166Z",
     "start_time": "2023-11-10T12:43:49.961112Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "file_ids = brown.fileids()\n",
    "txt = brown.raw(file_ids[0]) # any text\n",
    "# txt = '''Once the data has been acquired, it needs to be cleaned. Mostly, the data will contain duplicate entries, errors, or be inconsistent.Data pre-processing is an important step before applying any machine learning model. Same with the text data, before applying any machine learning model on text data, it requires data pre-processing. The pre-processing of text means cleaning of noise such as: removing stop words, punctuation, terms which doesnâ€™t carry much weightage in context to the text, etc. In this article, we describe in detail how to pre-process text data for machine learning algorithms using Python(NLTK).'''\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "# a&b\n",
    "def strToWordFreq(string: str):\n",
    "    import re\n",
    "    pattern = r'[\\w]+'\n",
    "    token_words = nltk.word_tokenize(string)\n",
    "    token_words = [word.lower() for word in token_words if re.findall(pattern, word)]\n",
    "    words = [tuple(word.split('/'))[0] for word in token_words]\n",
    "    return dict(nltk.FreqDist(words)) \n",
    "\n",
    "# c&d\n",
    "def txtToSents(txt: str):\n",
    "    sentsLen = len(nltk.sent_tokenize(txt))\n",
    "    return nltk.sent_tokenize(txt), sentsLen\n",
    "    \n",
    "sents, size = txtToSents(txt)\n",
    "\n",
    "def sentScore(sents):\n",
    "    score = {}\n",
    "    for sent in sents:\n",
    "        score[sent] = sum(strToWordFreq(sent).values())\n",
    "    return sorted(score.items(), key=lambda item: item[1], reverse=True)\n",
    "score = sentScore(sents)\n",
    "# e\n",
    "def summary(score, size, ratio=0.4):\n",
    "    if size < 10:\n",
    "        summ = ''\n",
    "        for sent in score:\n",
    "            summ = summ + (sent[0] + '\\n')\n",
    "        return summ\n",
    "    n = int(size * ratio)\n",
    "    summ = ''\n",
    "    for index, item in enumerate(score):\n",
    "        summ = summ + (item[0] + '\\n\\n')\n",
    "        if index == n:\n",
    "            return summ\n",
    "# print(summary(score, size))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849901e3-f58e-4e3d-ab1a-655c1e4877db",
   "metadata": {},
   "source": [
    "<b>Question 6</b><br>\n",
    "Write a function <code>shorten(text, n)</code> to process a text, omitting the n most frequently occurring words of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b8a2f1-7f7e-41a2-85d2-dc0ede6c909c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.029077Z",
     "start_time": "2023-11-10T12:43:50.020927Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileid = nltk.corpus.gutenberg.fileids()[5]\n",
    "txt = nltk.corpus.gutenberg.raw(fileid)\n",
    "\n",
    "def shorten(text, n):\n",
    "    import re\n",
    "    # process\n",
    "    pattern = r'[\\w][\\w]'  \n",
    "    words = [word for word in nltk.word_tokenize(txt) if re.findall(pattern, word)]\n",
    "    freq_dist = nltk.FreqDist(w.lower() for w in words)\n",
    "    # https://www.freecodecamp.org/news/sort-dictionary-by-value-in-python/\n",
    "    sort_by_value = sorted(freq_dist.items(), key=lambda item: item[1], reverse=True)\n",
    "    word = sort_by_value[n - 1][0]\n",
    "    pattern = word + ' ' # example: if word is 'the', to remove only the not the in 'their'\n",
    "    replaced = text.replace(pattern, '',)\n",
    "    return replaced\n",
    "\n",
    "# shorten(txt, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e6eb1-aba5-41ad-bbb1-fbc2a1035e79",
   "metadata": {},
   "source": [
    "<b>Question 7</b><br>\n",
    "With the help of the trie data structure, write a recursive function that processes text, locating the uniqueness point in each word, and discarding the remainder of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6709be7-3898-4aee-b2ff-b1743fbb9bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.524595Z",
     "start_time": "2023-11-10T12:43:50.029892Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13192/1233806695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# remove stopwords, punctuations, whitespaces, frequent words etc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'[\\w]+'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtoken_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.util import Trie # https://tedboy.github.io/nlps/generated/generated/nltk.Trie.html\n",
    "trie_struct = Trie()\n",
    "path = 'Question9.txt'\n",
    "with open(file=path, encoding='UTF8') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove stopwords, punctuations, whitespaces, frequent words etc\n",
    "pattern = r'[\\w]+'\n",
    "token_words = word_tokenize(txt)\n",
    "words = [word.lower() for word in token_words if word.lower() not in stop and re.findall(pattern, word)]\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c56a0d-f5f3-4955-87a1-cd5dbe1f6fca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.526578Z",
     "start_time": "2023-11-10T12:43:50.526578Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    trie_struct.insert(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69c0b9-de73-4615-b36f-fc9fee913e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.527561Z",
     "start_time": "2023-11-10T12:43:50.527561Z"
    }
   },
   "outputs": [],
   "source": [
    "trie_struct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942e7f6-ad2d-41c7-ada5-a500e0e0c70e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:43:50.529575Z",
     "start_time": "2023-11-10T12:43:50.529575Z"
    }
   },
   "outputs": [],
   "source": [
    "trie_struct.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9324b9b-ea68-438f-a889-e0887f559b7c",
   "metadata": {},
   "source": [
    "<b>Question 8</b><br>\n",
    "Write code to search the Brown Corpus for particular words and phrases according to tags, to answer the following questions:\n",
    "\n",
    "a. Produce an alphabetically sorted list of the distinct words tagged as MD.\n",
    "\n",
    "b. Identify words that can be plural nouns or third person singular verbs.\n",
    "\n",
    "c. Identify three-word prepositional phrases of the form IN + DET + NN.\n",
    "\n",
    "d. What is the ratio of masculine to feminine pronouns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09318bfb-0032-4a81-8a10-c79326e43f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:23.886489Z",
     "start_time": "2023-11-10T12:44:15.047008Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk    \n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "# https://www.nltk.org/api/nltk.probability.ConditionalFreqDist.html\n",
    "words = brown.words()\n",
    "tagged_words = brown.tagged_words()\n",
    "set_words = set(words)\n",
    "cfd = ConditionalFreqDist(tagged_words) # unpack tuples and count them \n",
    "conditions = cfd.conditions()\n",
    "# conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d95b7aaa-b44a-45e1-a93a-ff119a32cf05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.442958Z",
     "start_time": "2023-11-10T12:44:23.889204Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a\n",
    "MD = []\n",
    "MD.extend((item[0].lower(), item[1]) for item in tagged_words if item[1] == 'MD')\n",
    "setMd = sorted(set(MD)) # https://learnpython.com/blog/sort-alphabetically-in-python/\n",
    "# setMd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58ce7a28-0d3b-4f13-ba38-01de410eb17f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.489375Z",
     "start_time": "2023-11-10T12:44:26.445628Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b. Identify words that can be plural nouns or third person singular verbs.\n",
    "pAnd3rd = [condition for condition in conditions if cfd[condition]['NNS'] and cfd[condition]['VBZ']]\n",
    "pAnd3rd.sort()\n",
    "# print(pAnd3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f04ec69-f409-448d-a2b6-efa9711904dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.497310Z",
     "start_time": "2023-11-10T12:44:26.492328Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c. Identify three-word prepositional phrases of the form IN + DET + NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe754695-f6ca-46c4-b843-484704aa7490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.518255Z",
     "start_time": "2023-11-10T12:44:26.499305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3384615384615386"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d. What is the ratio of masculine to feminine pronouns?\n",
    "f = sum(cfd['She'].values()) + sum(cfd['she'].values())\n",
    "m = sum(cfd['he'].values()) + sum(cfd['He'].values())\n",
    "m / f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202f938-4c83-41ab-896b-eb096909ddf2",
   "metadata": {},
   "source": [
    "<b>Question 9</b><br>\n",
    "First, read the text from Question9.txt file with appropriate encoding , then preprocess the text and tokenize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08f27d27-56be-42e6-ad9b-8bc5bb8fe9b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.532222Z",
     "start_time": "2023-11-10T12:44:26.520248Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf53e85-e3e0-427c-acec-e9b03535c204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:44:26.553198Z",
     "start_time": "2023-11-10T12:44:26.533215Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = 'Question9.txt'\n",
    "with open(file=path, encoding='UTF8') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# remove stopwords, punctuations, whitespaces, frequent words etc\n",
    "pattern = r'[\\w]+'\n",
    "token_words = word_tokenize(txt)\n",
    "words = [word.lower() for word in token_words if word.lower() not in stop and re.findall(pattern, word)]\n",
    "words = set(words)\n",
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1ada-ced9-4f68-a55e-f5ac89823db5",
   "metadata": {},
   "source": [
    "<b>Question 10</b><br>\n",
    "With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus , and NLTK's frequency distribution and rank correlation functionality , develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf0a2f-100e-4350-a717-bc356ac0efe0",
   "metadata": {},
   "source": [
    "<b>Question 11</b><br>\n",
    "Consider a city that is structured as a grid of intersections (each intersection can be thought of as a cell in a 2D list). A dog is positioned randomly at one of the intersections. The dog can move upwards, downwards, left, or right, and it cannot pass through the same intersection more than once. Write a program to simulate the random movements of the dog in this city. Calculate the probability of the dog exiting the city based on the given rules. To calculate the probability, you need to run the program a large number of times (for example, ten thousand times) and divide the number of times the dog successfully exits the grid by the total number of executions. This method of probability calculation is called Monte Carlo simulation. The dimensions of the city are taken from the user (number of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74eaa9e4-19bc-46f0-9ef5-27c103ffd022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:45:03.160028Z",
     "start_time": "2023-11-10T12:45:02.476927Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3838\n"
     ]
    }
   ],
   "source": [
    "def city(row, col):\n",
    "    return [[0 for x in range(c)] for y in range(r)] \n",
    "    \n",
    "def point(row, col):\n",
    "    from random import randrange\n",
    "    rowPoint = randrange(row)\n",
    "    colPoint = randrange(col)\n",
    "    return rowPoint, colPoint\n",
    "\n",
    "def move(currentRow,\n",
    "         currentCol, \n",
    "         moves={0: (-1, 0),\n",
    "         1: (1, 0),\n",
    "         2: (0, 1),\n",
    "         3: (0, -1)},\n",
    "        random=True,\n",
    "        moveCode=0):\n",
    "    if random:\n",
    "        from random import randrange\n",
    "        moveCode = randrange(0, 4)\n",
    "        moveRow = moves[moveCode][0]\n",
    "        moveCol = moves[moveCode][1]\n",
    "        return currentRow + moveRow, currentCol + moveCol, moveCode\n",
    "    moveRow = moves[moveCode][0]\n",
    "    moveCol = moves[moveCode][1]\n",
    "    return currentRow + moveRow, currentCol + moveCol, moveCode\n",
    "\n",
    "\n",
    "def isValid(city2D, nextRow, nextCol, size: tuple):\n",
    "    if nextRow >= 0 and nextRow < size[0] and nextCol >= 0 and nextCol < size[1]:\n",
    "        if city2D[nextRow][nextCol] != 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def intiat(row, col):\n",
    "    city2D = city(row=r, col=c)\n",
    "    startRow, startCol = point(row=r, col=c)\n",
    "    exitRow, exitCol = point(row=r, col=c)\n",
    "    while startRow == exitRow or startCol == exitCol:\n",
    "        exitRow, exitCol = point(row=r, col=c)\n",
    "    currentRow, currentCol = startRow, startCol\n",
    "    city2D[currentRow][currentCol] = 1\n",
    "    city2D[exitRow][exitCol] = -1\n",
    "    return city2D, currentRow, currentCol \n",
    "    \n",
    "def cityAndDog(city2D, currentRow, currentCol):\n",
    "    while city2D[currentRow][currentCol] != -1:\n",
    "        nextRow, nextCol, moveCode = move(currentRow=currentRow, currentCol=currentCol)\n",
    "        moveCode = -1\n",
    "        while not isValid(city2D=city2D, nextRow=nextRow, nextCol=nextCol, size=size):\n",
    "            moveCode += 1\n",
    "            random = False\n",
    "            if moveCode == 4:\n",
    "                # print(random,moveCode) \n",
    "                return True\n",
    "            nextRow, nextCol, moveCode = move(currentRow=currentRow,\n",
    "                                              currentCol=currentCol,\n",
    "                                              random=random,\n",
    "                                              moveCode=moveCode)\n",
    "            # print(random,moveCode)      \n",
    "        currentRow = nextRow\n",
    "        currentCol = nextCol\n",
    "        if city2D[currentRow][currentCol] == -1:\n",
    "            city2D[currentRow][currentCol] = -2\n",
    "            break\n",
    "        city2D[currentRow][currentCol] = 1\n",
    "\n",
    "def randomTest(n=10000):\n",
    "    blocked = 0\n",
    "    for i in range(n):\n",
    "        city2D, currentRow, currentCol = intiat(row=r, col=c)\n",
    "        block = cityAndDog(city2D=city2D, currentRow=currentRow, currentCol=currentCol)\n",
    "        if block:\n",
    "            blocked += 1\n",
    "    return n - blocked, blocked\n",
    "\n",
    "# ## user input\n",
    "# valid = False\n",
    "# while not valid:\n",
    "#     r, c = eval(input('enter row (row >= 2) and col (col >= 2):'))\n",
    "#     if r <= 1 or c <= 1:\n",
    "#         continue\n",
    "#     else:\n",
    "#         valid = True \n",
    "\n",
    "r, c = 8, 8\n",
    "size = (r, c)\n",
    "n = 10000\n",
    "exit, blocked = randomTest(n)\n",
    "# print(exit, blocked)\n",
    "print(exit / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c5e59-9a70-410e-9e9f-4f9dcf2d94af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
